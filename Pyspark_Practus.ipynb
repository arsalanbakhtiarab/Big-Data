{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1531d95-5100-4587-bf82-3369abbb6957",
   "metadata": {},
   "source": [
    "# PySpark v3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0fe36-331d-4407-bd62-09837d2cd244",
   "metadata": {},
   "source": [
    "# Spark Overview\n",
    "#### Apachi spark is a fast and genereal purpose cluster computing system. it provide high level API's in Scala, java, python that make parrallel jobs easy to write, and optimized engine that supports general computation graphs. It also supports a rich set of higher-level tools including Shark (Hive on spark), MLib for machinelearning, GraphX for graph processing, and Spark Streaming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef437b-52e7-4147-add2-3b5ca63fc452",
   "metadata": {},
   "source": [
    "## Speed\n",
    "#### Run Workloads 100x Speed.\n",
    "## Ease Of Use\n",
    "#### Write Application quickly in java, Scala, Python, R and SQL.\n",
    "## Generality\n",
    "#### Combine SQL, Streaming and complex analytics(Mlib : Machine learning library, GraphX).\n",
    "## Run Everywhere\n",
    "#### Spark can runs on Hadoop, Apache Mesos, Kubemeters, Standalone or in the clould.\n",
    "#### it can access diverse data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab93f26a-097c-4438-b513-d61642c97486",
   "metadata": {},
   "source": [
    "## Install pyspark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bcb5cf-37c5-49bb-8e48-afa25bd431c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\arsalan bakhtiar ab\\appdata\\roaming\\python\\python312\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\arsalan bakhtiar ab\\appdata\\roaming\\python\\python312\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2fa50-3366-478d-9546-44471830741d",
   "metadata": {},
   "source": [
    "## Import pyspark libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdc3ea8-2055-4dca-87eb-607ba4b3c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version : 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(\"Version :\",pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4de10-7139-43f3-87a8-ff60d4ad21fc",
   "metadata": {},
   "source": [
    "## Read file with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5ea71c-699c-49f3-b6f7-725a06cff36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arsalan</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bakhtiar</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AB</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azman</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Furqan</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Afzal</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  age  Experience  Salary\n",
       "0   Arsalan   31          10   30000\n",
       "1  Bakhtiar   30           8   25000\n",
       "2        AB   29           4   20000\n",
       "3     Azman   24           3   20000\n",
       "4    Furqan   21           1   15000\n",
       "5     Afzal   23           2   18000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ce1a3-ba81-4040-a9ca-ca9f0c3b3c42",
   "metadata": {},
   "source": [
    "## Note: \n",
    "#### When want to work with pyspark always, first of all you need to start a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074f07c3-a2d2-4c3c-8cd5-6f42a6db8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51f1dd-ecea-4946-a9db-e09dc32a4947",
   "metadata": {},
   "source": [
    "## Creating a instance of a SparkSession Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9297528-f16c-4629-b479-b05c52d2b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a67978-63fb-49e9-a49c-d7ae07728ec7",
   "metadata": {},
   "source": [
    "## Note:\n",
    "### Executing on local machine there is only one cluster, when you working on clould you can create multiple cluster or instances ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93206f13-624d-4f2e-9f0b-d07e51db119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-S1FT3E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1727b277cb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ea94b-bcd2-41d4-a534-ef6a845daa57",
   "metadata": {},
   "source": [
    "## Read Dataset With PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa7e8ca-b7e3-4ede-8204-657d61869f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be77a12a-79b7-4e77-9062-4b715362ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|     _c0|_c1|       _c2|   _c3|\n",
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc92c6-300f-4bc2-bf8c-3d61d4888296",
   "metadata": {},
   "source": [
    "#### To get the defalut header name of the Dataset instead of just random name like _c0, _c1 ... cn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ffa498-f6df-4b18-b6a0-5a6122c170f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv').show()\n",
    "# or\n",
    "df_pyspark = spark.read.csv('test1.csv',header = True , inferSchema = True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d61063-3e90-4615-8e71-bf5e9e8eb154",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Addind \n",
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c844d8b-ef7a-460d-a495-7dab45d42356",
   "metadata": {},
   "source": [
    "## Pyspark Dataframe\n",
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be947cb8-1a2c-44d1-a410-1e9cf901e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d35c7-ca88-4457-9bce-ab67da4dd4ae",
   "metadata": {},
   "source": [
    "## Schema\n",
    "#### It can show the details about the dataset. Cheking the Datatype of the columns. Also show if there is any null value.\n",
    "## Note:\n",
    "#### if you can used the .show() at the end so id doesn't work.\n",
    "#### You can also define one more option the csv() is \"inferSchema = True\" . Other wisre it can be consider all the vlaues as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2908be0b-a679-4438-967f-73bf3dae79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv',inferSchema = True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11679308-80e9-463d-9cd2-c5abe164f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv',header = True , inferSchema = True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86ad86-fae8-4e9c-a169-3e8860f65d43",
   "metadata": {},
   "source": [
    "## Check the type of the Dataframe\n",
    "## Note :\n",
    "### Dataframe is a Data Structure, Because inside this you can perform various kind of operations. so this is also one kind of Data Structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46f9d246-d8b4-4ee1-bba1-7e097c4b9a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5dab0-3a1a-44e1-8aca-a6cdcbc8af25",
   "metadata": {},
   "source": [
    "## Dealing with Columns\n",
    "- 01 : Print the first 3 rows\n",
    "- 02 : print the columns name only.\n",
    "- 03 : Show all the columns .show()\n",
    "- 04 : Select the entire column one or multiple .select('Name of the column','Second column name')\n",
    "- 05 : Check the Data type .dtype\n",
    "- 06 : To check mean, median, mode and stuf like this used .describe()\n",
    "- 07 : Adding columns .withColumns()\n",
    "- 08 : Drop the columns .drop() drop single or multiple\n",
    "- 09 : Remane the column  .withColumnRenamed('Existance','New Name')\n",
    "- 06 : To check mean, median, mode and stuf like this used .describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a737c8-37aa-438a-b00f-53753c114e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Arsalan', age=31, Experience=10, Salary=30000),\n",
       " Row(Name='Bakhtiar', age=30, Experience=8, Salary=25000),\n",
       " Row(Name='AB', age=29, Experience=4, Salary=20000)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca9abc56-aa6e-4275-b1ba-aa2893c69678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b823c21f-0eae-4d8c-86f5-97e535f49fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3ea39e-a3b2-40c7-9944-d99365d74e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "| Arsalan|\n",
      "|Bakhtiar|\n",
      "|      AB|\n",
      "|   Azman|\n",
      "|  Furqan|\n",
      "|   Afzal|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe5aed7-28b5-4302-9254-b214b44daf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    Name|Salary|\n",
      "+--------+------+\n",
      "| Arsalan| 30000|\n",
      "|Bakhtiar| 25000|\n",
      "|      AB| 20000|\n",
      "|   Azman| 20000|\n",
      "|  Furqan| 15000|\n",
      "|   Afzal| 18000|\n",
      "+--------+------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Salary']).show()\n",
    "print (type(df_pyspark.select(['Name','Salary'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35c08ab-b4a3-4158-a599-68911dd9f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Arsalan| 31|\n",
      "|Bakhtiar| 30|\n",
      "|      AB| 29|\n",
      "|   Azman| 24|\n",
      "|  Furqan| 21|\n",
      "|   Afzal| 23|\n",
      "+--------+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_pyspark['Name','Age'].show()\n",
    "print (type(df_pyspark['Name',]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156f4963-c425-4ab6-aa82-9e2be271baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 05 : Check the Data type .dtype\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d3701c5-a205-4760-86a8-d61fa24a1ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|               Age|       Experience|            Salary|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|                 6|                6|                 6|\n",
      "|   mean|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|                21|                1|             15000|\n",
      "|    max|                31|               10|             30000|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 06 : To check mean, median, mode and stuf like this used .describe()\n",
    "df_pyspark['Age','Experience','Salary'].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85d8de64-69ab-4d61-9105-534563c50b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+-----------------------+\n",
      "|    Name|age|Experience|Salary|Experience After 2 year|\n",
      "+--------+---+----------+------+-----------------------+\n",
      "| Arsalan| 31|        10| 30000|                     12|\n",
      "|Bakhtiar| 30|         8| 25000|                     10|\n",
      "|      AB| 29|         4| 20000|                      6|\n",
      "|   Azman| 24|         3| 20000|                      5|\n",
      "|  Furqan| 21|         1| 15000|                      3|\n",
      "|   Afzal| 23|         2| 18000|                      4|\n",
      "+--------+---+----------+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 07 : Adding columns .withColumns()\n",
    "df_pyspark.withColumn('Experience After 2 year',df_pyspark['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f796526-f676-4457-aef0-f4355eca1f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 08 : Drop the columns .drop() drop single or multiple\n",
    "df_pyspark.drop('Experience After 2 year')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676a8c5f-a946-4d8b-8046-585582819dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|New Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 09 : Remane the column  .withColumnRenamed('Existance','New Name')\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44c27c-da60-4f08-9c09-c5a33c06b4cf",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf1807-c08f-43a6-afd9-4e1e786863fa",
   "metadata": {},
   "source": [
    "- Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26de6908-53b5-45e7-9ded-d690af99472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc99380-0889-4827-8753-c851c00d578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Handling Missing Values\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6edf0f2d-b295-46bc-b510-3ed720af8e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "| Arsalan|  31|        10| 30000|\n",
      "|Bakhtiar|  30|         8| 25000|\n",
      "|      AB|  29|         4| 20000|\n",
      "|   Azman|  24|         3| 20000|\n",
      "|  Furqan|  21|         1| 15000|\n",
      "|   Afzal|  23|         2| 18000|\n",
      "|   Talha|NULL|      NULL| 40000|\n",
      "|    NULL|  34|        10| 38000|\n",
      "|    NULL|  36|      NULL|  NULL|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test2.csv',header=True,inferSchema = True).show()\n",
    "df = spark.read.csv('test2.csv',header=True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4816b-4249-4738-b8e1-111674a0d332",
   "metadata": {},
   "source": [
    "- Drop the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3626919f-a517-4448-b40b-37530d7c9bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|    Name|Experience|Salary|\n",
      "+--------+----------+------+\n",
      "| Arsalan|        10| 30000|\n",
      "|Bakhtiar|         8| 25000|\n",
      "|      AB|         4| 20000|\n",
      "|   Azman|         3| 20000|\n",
      "|  Furqan|         1| 15000|\n",
      "|   Afzal|         2| 18000|\n",
      "|   Talha|      NULL| 40000|\n",
      "|    NULL|        10| 38000|\n",
      "|    NULL|      NULL|  NULL|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c87d4-ad2a-4097-9425-4f49ef87bde1",
   "metadata": {},
   "source": [
    "- Dropping Rows\n",
    "- Various Parameter in Dropping functionalities\n",
    "> how = 'any' # if 'any', drop a row if contains any nulls.\n",
    "> how = 'all' # if 'all', drop a row only if all its values are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2294c4b4-a6c1-4273-8d83-4ab100f1043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "| Arsalan|  31|        10| 30000|\n",
      "|Bakhtiar|  30|         8| 25000|\n",
      "|      AB|  29|         4| 20000|\n",
      "|   Azman|  24|         3| 20000|\n",
      "|  Furqan|  21|         1| 15000|\n",
      "|   Afzal|  23|         2| 18000|\n",
      "|   Talha|NULL|      NULL| 40000|\n",
      "|    NULL|  34|        10| 38000|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270127b-9aa2-42c7-9130-54bae84e8b6d",
   "metadata": {},
   "source": [
    "- Threshold (thresh = 2) # it can be check atleast if there is 2 non-null value be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cf64085-166c-4630-9916-2d6a6c98529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "| Arsalan|  31|        10| 30000|\n",
      "|Bakhtiar|  30|         8| 25000|\n",
      "|      AB|  29|         4| 20000|\n",
      "|   Azman|  24|         3| 20000|\n",
      "|  Furqan|  21|         1| 15000|\n",
      "|   Afzal|  23|         2| 18000|\n",
      "|   Talha|NULL|      NULL| 40000|\n",
      "|    NULL|  34|        10| 38000|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all',thresh= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3c7c7-e20c-42e7-9c42-37fbb9e718bf",
   "metadata": {},
   "source": [
    "- Subset subset=[] # Drop the nan values only from a specific Columns.it can be delete the whole row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "501a1e4b-d7df-471f-b614-44062f117717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "|    NULL| 34|        10| 38000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6b09e-3376-4aa8-83e9-0f4890130cc0",
   "metadata": {},
   "source": [
    "- Filling the Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "502b3eb4-d64a-4021-9fdc-c289697994fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|       Arsalan|  31|        10| 30000|\n",
      "|      Bakhtiar|  30|         8| 25000|\n",
      "|            AB|  29|         4| 20000|\n",
      "|         Azman|  24|         3| 20000|\n",
      "|        Furqan|  21|         1| 15000|\n",
      "|         Afzal|  23|         2| 18000|\n",
      "|         Talha|NULL|      NULL| 40000|\n",
      "|Missing_Values|  34|        10| 38000|\n",
      "|Missing_Values|  36|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('Missing_Values',['Name','age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1562697-b058-4e53-b678-73f92e00a626",
   "metadata": {},
   "source": [
    "- Handling Missing values by Mean, Median & Mode (Imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250aab46-ad67-47c8-8196-b79b28d7fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce0f7a2b-07fb-41b5-a0fa-7690b597820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols = ['age', 'Experience', 'Salary'], \n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "                 ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d744e00-bc97-446c-ac89-d712d424b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "| Arsalan|  31|        10| 30000|         31|                10|         30000|\n",
      "|Bakhtiar|  30|         8| 25000|         30|                 8|         25000|\n",
      "|      AB|  29|         4| 20000|         29|                 4|         20000|\n",
      "|   Azman|  24|         3| 20000|         24|                 3|         20000|\n",
      "|  Furqan|  21|         1| 15000|         21|                 1|         15000|\n",
      "|   Afzal|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Talha|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|    NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|    NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "+--------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68893a50-580c-475b-917a-c49060e13a16",
   "metadata": {},
   "source": [
    "## Filter Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5d65b-8c34-490b-b2cb-b482d104ee7d",
   "metadata": {},
   "source": [
    "- Filter Operation are\n",
    "- & | ==\n",
    "- Inverse Condition ( ~ ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9aac4e7-70f3-4fd3-b38e-04b81593dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0519e24-4c8d-4b0a-8fe1-ddc9b748ef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-S1FT3E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Filter Operation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21d57f975f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Filter Operation').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b00a7f7-ff31-4763-bbdd-1fd42ed01dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test1.csv',header = True,inferSchema = True ).show()\n",
    "df = spark.read.csv('test1.csv',header = True,inferSchema = True )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bddbc8-adca-472a-ade8-e5b41208fe15",
   "metadata": {},
   "source": [
    "- Salary of the people less than or equal to 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "066652de-4c8d-4bbc-a660-33293ccaa7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Salary<=25000\").show()\n",
    "# or \n",
    "df.filter(df[\"Salary\"]<=25000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be0ae441-d168-4071-bc28-41000de89642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|    AB| 29|         4| 20000|\n",
      "| Azman| 24|         3| 20000|\n",
      "|Furqan| 21|         1| 15000|\n",
      "| Afzal| 23|         2| 18000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the specific columns.\n",
    "df.filter(\n",
    "            (df[\"Salary\"]<=20000) & (df[\"Salary\"]>=15000)\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1adad8e-2197-4e8a-aff0-a71f81af35b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inverse Condition.\n",
    "df.filter(~(\n",
    "            (df[\"Salary\"]<=20000) & (df[\"Salary\"]>=15000)\n",
    "         )).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09d11f-5542-4104-98c4-6901e8ed0048",
   "metadata": {},
   "source": [
    "## GroupBy & Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6abc58-7a79-435f-bdd8-3d35449d4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c886a2-9b9a-4289-9b1b-fe8d56ef852b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-S1FT3E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Filter Operation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21d57f975f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('GroupBy_&_Aggregate').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6ca2c9-4bbe-48c7-a63c-27bacb3c6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|    Name| Departments|salary|\n",
      "+--------+------------+------+\n",
      "| Arsalan|Data Science| 10000|\n",
      "|Bakhtiar|         IOT|  5000|\n",
      "| Arsalan|    Big Data|  4000|\n",
      "|   Azman|    Big Data|  4000|\n",
      "|  Furqan|Data Science|  3000|\n",
      "|   Afzal|Data Science| 20000|\n",
      "|   Talha|         IOT| 10000|\n",
      "|  Salman|    Big Data|  5000|\n",
      "|   Roman|Data Science| 10000|\n",
      "|   Usman|    Big Data|  2000|\n",
      "+--------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"test3.csv\",header = True,inferSchema = True).show()\n",
    "df = spark.read.csv(\"test3.csv\",header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff577095-4b3b-43a4-a0a3-e0d30df32321",
   "metadata": {},
   "source": [
    "- GroupBy Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c08da727-9eb8-46c6-95a5-24df35ccd6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|sum(salary)|\n",
      "+--------+-----------+\n",
      "|  Salman|       5000|\n",
      "|   Afzal|      20000|\n",
      "|   Azman|       4000|\n",
      "|   Talha|      10000|\n",
      "|   Roman|      10000|\n",
      "|  Furqan|       3000|\n",
      "|Bakhtiar|       5000|\n",
      "| Arsalan|      14000|\n",
      "|   Usman|       2000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f41c79-f2e2-42ed-b6fb-a5f83884bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|    Name|max(salary)|\n",
      "+--------+-----------+\n",
      "|  Salman|       5000|\n",
      "|   Afzal|      20000|\n",
      "|   Azman|       4000|\n",
      "|   Talha|      10000|\n",
      "|   Roman|      10000|\n",
      "|  Furqan|       3000|\n",
      "|Bakhtiar|       5000|\n",
      "| Arsalan|      10000|\n",
      "|   Usman|       2000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maximum Salary\n",
    "df.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d357d5d8-528b-4748-bc35-3a44ca00b27b",
   "metadata": {},
   "source": [
    "- GroupBy Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2c1478f-e6bb-442b-bcf2-c455ab3b43f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659a3aa-d737-41c5-a164-cc135b1462ec",
   "metadata": {},
   "source": [
    "- Count people in Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d9cd275-55d7-49d2-814d-bdb357ea5342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a0d29-645f-4dc4-a1ea-4340d5afad60",
   "metadata": {},
   "source": [
    "- Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33d4cdf-28a5-4a1b-9b89-e60d08f707ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd805e-90c2-46d8-b9ba-1e97f94438cd",
   "metadata": {},
   "source": [
    "## Note\n",
    "- Spark ML has two different techniques one is RDD API technique and the second one is Data freame API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d076f6e-297d-4674-8a98-7eea21dc0e3c",
   "metadata": {},
   "source": [
    "## PySpark Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b68577-c848-46d4-8fbc-dbe7e901d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f3ca1c-d564-4ac3-9beb-de6f61d198fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Py Spark Machine Learning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964bf36d-1ed5-467e-8a1a-c2b4a64147db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-S1FT3E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Py Spark Machine Learning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2127ee47a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d096cf-08c0-4295-bdb0-6dd1b397c07f",
   "metadata": {},
   "source": [
    "- Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75f64cb-c177-40a8-8e09-e14bc8626470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "| Arsalan| 31|        10| 30000|\n",
      "|Bakhtiar| 30|         8| 25000|\n",
      "|      AB| 29|         4| 20000|\n",
      "|   Azman| 24|         3| 20000|\n",
      "|  Furqan| 21|         1| 15000|\n",
      "|   Afzal| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.read.csv(\"test1.csv\",header = True, inferSchema=True)\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea281327-62bb-47c4-ae17-8e26aea082bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42cabd18-97bb-41ab-bfb6-bcd56630146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9096c94-8aad-4f88-a1f6-289ffa897aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a85446-212c-45b0-9520-008b211e8409",
   "metadata": {},
   "source": [
    "- Note\n",
    "- ['age', 'Experience'] --> New Feature --> Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f117986-5d13-4dce-a887-a526afdfb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler = VectorAssembler(inputCols=['age','Experience'],outputCol='Independet Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e502a0-632a-4ebc-bdcd-2cb6ee33b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca35875f-8243-48e8-b867-49e906f8476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+-------------------+\n",
      "|    Name|age|Experience|Salary|Independet Features|\n",
      "+--------+---+----------+------+-------------------+\n",
      "| Arsalan| 31|        10| 30000|        [31.0,10.0]|\n",
      "|Bakhtiar| 30|         8| 25000|         [30.0,8.0]|\n",
      "|      AB| 29|         4| 20000|         [29.0,4.0]|\n",
      "|   Azman| 24|         3| 20000|         [24.0,3.0]|\n",
      "|  Furqan| 21|         1| 15000|         [21.0,1.0]|\n",
      "|   Afzal| 23|         2| 18000|         [23.0,2.0]|\n",
      "+--------+---+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad60d04-5511-431e-b93b-e47f10e45c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary', 'Independet Features']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf6077ee-a052-4919-818b-5dacd9c71dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finilize_data = output.select('Independet Features','Salary' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b476be8-33d3-4513-99f1-9f839b4b6bb4",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c7a6ad-ba4a-4feb-a000-ec8f710ff13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef9aad-befc-4885-ae05-a43df43aebe5",
   "metadata": {},
   "source": [
    "- Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f01371-192f-4fd3-9015-2e70e40750fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = finilize_data.randomSplit([0.75,0.25])\n",
    "reg = LinearRegression(featuresCol='Independet Features',labelCol='Salary')\n",
    "reg = reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f03d0-b908-491c-82e3-4fd72b7eb814",
   "metadata": {},
   "source": [
    "- Cofficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cc93515-cbf1-4e70-9939-aa071a1e1583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-263.7076, 1767.624])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b6130-2836-4a73-aea1-fb2904ca0e82",
   "metadata": {},
   "source": [
    "- Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceaf7c63-f93d-44d0-8431-12ab47344f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19919.060052212404"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f08400-1f22-4749-ae62-df9af08571d7",
   "metadata": {},
   "source": [
    "- Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "226fd9eb-0ed6-407b-ade6-972a786315e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = reg.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c314c502-0891-4f81-89f9-e23997ddc5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------------+\n",
      "|Independet Features|Salary|       prediction|\n",
      "+-------------------+------+-----------------+\n",
      "|         [29.0,4.0]| 20000|19342.03655352618|\n",
      "+-------------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab898e7-78bf-48a1-bb09-0e6a2661da9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657.9634464738192, 432915.89689570636)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb56219-6bf1-40b4-93a6-3516edcc9d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140bc78-3b5d-45f4-a1e8-74791dd30f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a608a1b9-334b-4c33-961b-176574f354f3",
   "metadata": {},
   "source": [
    "## Class Room Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93187529-9f82-45fd-93f2-8b030a276a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee600449-7a84-4596-b738-d902329de98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1b9796-4871-4a6c-bc0c-9f1065ab8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c19d927-ceb0-4f6c-9fcf-5883f335555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Square(x) -> int:\n",
    "    y  = x*x\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d06a14-cd58-4048-8cf1-6a496dc8ced1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230caff5-9326-4203-8df0-e098ce3af068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4,5]\n",
    "for i in list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73edaa4-40b9-4c73-9d8c-ddbb82f55ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4,5,6]\n",
    "for i in list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b38b35a-0679-4f82-a08d-b9bdf1348990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"BroadcastExample\").getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create a variable to be broadcasted\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Broadcast the variable\n",
    "broadcast_data = sc.broadcast(data)\n",
    "\n",
    "# Access the broadcasted value\n",
    "broadcast_value = broadcast_data.value\n",
    "\n",
    "# Print the broadcasted value\n",
    "print(broadcast_value)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d698f-3edb-40de-a439-58d257b970fe",
   "metadata": {},
   "source": [
    "- Working With Key value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ba09b18-c2bb-4359-b271-b8e5bc731178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921f1694-f3c1-41d2-a882-200e577972b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa768a7c-1e46-426a-b5c3-5feba8b474a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5ee5a5-8dc8-4e2d-a7e8-eeb14f37f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ML on Tip data set').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc692e1-f51a-4301-a2b8-b3b43bcebf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-S1FT3E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ML on Tip data set</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1957f3e3650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a9390e-258b-4102-b702-db2b8fa2883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('tips.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f90ce-46cb-490e-9b5d-c85aee6541b7",
   "metadata": {},
   "source": [
    "### Word Count using map reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8456ff-4d34-4020-89cf-646bb8127b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f7866c-ae02-4eb8-be38-e3624558df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intilize Spark context\n",
    "spark = SparkSession.builder.appName(\"Word Count\").config(\"spark.driver.memory\", \"4g\").getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"Word Count\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18099ae4-d8a1-41ff-aaa9-1715b7ad6271",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 1 times, most recent failure: Lost task 6.0 in stage 0.0 (TID 6) (10.10.69.56 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m reduced_rdd \u001b[38;5;241m=\u001b[39m mapped_rdd\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x,y:x\u001b[38;5;241m+\u001b[39my)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Collect the result\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mreduced_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 1 times, most recent failure: Lost task 6.0 in stage 0.0 (TID 6) (10.10.69.56 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "# Intilize Spark context\n",
    "spark = SparkSession.builder.appName(\"Word Count\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark\n",
    "# Create RDD\n",
    "data = ['apple','banana','grape','banana','orange','apple']\n",
    "rdd = sc.parallelize(data)\n",
    "# Map Step: Assign a count of 1 to each string\n",
    "mapped_rdd = rdd.map(lambda x:(x,1))\n",
    "# Reduce Step: Sum the counts for each string\n",
    "reduced_rdd = mapped_rdd.reduceByKey(lambda x,y:x+y)\n",
    "# Collect the result\n",
    "result = reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e334f8-b0d1-4b89-8482-07f919fe25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD\n",
    "data = ['apple','banana','grape','banana','orange','apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02413b4-fbd7-426c-b202-28d2e3e19313",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50d12e6-385f-4f8e-8461-40e0b086f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Step: Assign a count of 1 to each string\n",
    "mapped_rdd = rdd.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8f78a6f-4683-42b5-b8b4-14ec71ba7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Step: Sum the counts for each string\n",
    "reduced_rdd = mapped_rdd.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08e403b0-1528-4607-91e1-aff5032dda3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (DESKTOP-S1FT3E0 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Collect the result\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mreduced_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 0.0 failed 1 times, most recent failure: Lost task 4.0 in stage 0.0 (TID 4) (DESKTOP-S1FT3E0 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "# Collect the result\n",
    "result = reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cd4d1-3b5e-4f16-b7a6-2797e9b791c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the result\n",
    "for (word,count) in result:\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
